<<<<<<< Updated upstream
=======
#   for those patients receiving heart transplants from 1992 through 1995 when the previous hospital control strategy was used;
#   G = 1 for those patients receiving heart transplants from 1996 through 1999 when the new hospital infection control strategy
#   was adopted. The primary event of interest is getting a hospital infection after surgery. A competing risk is death during
#   recovery from surgery without getting a hospital infection. Control variables being considered are tissue mismatch score (TMS)
#   at transplant and AGE at transplant. The outcome variable of interest is time (DAYS after surgery) until a patient developed
#   a hospital infection.
#27)State a cause-specific no-interaction Cox PH model for assessing the effect of group status (G) on time until a hospital infection
#   event.
# h(t,x)=hnull*exp(B1*TMS + B2*AGE + B3*G); deaths without infection are considered censored.
#28)When fitting hte model given in Question 27, which patients should be censored?
#   Patients who die without a hospital infection are considered censored.
#29)Describe or provide a table that would show how the data on the ith patient should be augmented for input into a Lunn-McNeil model
#   for analysis.
#   In a Lunn-McNeil model, the table would include two binary outcome events, one for each of the competing risks of interest.
#30)State a Lunn-McNeil model that can be used with an augmented dataset htaht will provide identical results to those obtained
#   from useing the model of Question 27.
#   h(t,x)=hnull*exp(B1*TMS + B2*AGE + B3*G + B4*D2*TMS + B5*D2*AGE + B6*D2*G);
#   D2=0 if patient develops an infection
#   D2=1 if patient dies without infection
#31)For the LM model of Question 30, what is the formula for the hazard ratio for the group effect G, controlling for TMS and AGE
#   haz=exp(B3*G); G=1
#32)Describe how you would test whether a no-interaction SC LM Model would be more appropriate than an interaction SC LM Model
#   You would compare the two with a likelihood ratio: the full model is that of question 30, the reduced model is that of
#   question 27. You would calculate the likelihood ratio and treat it as a chi-squared stat with 3 degrees of freedom and
#   use the p-value to accept or reject the null hypothesis that the interaction variables add nothing to the model.
#33)State a Lunn-McNeil model that can be used with an augmented dataset that will provide identical results to those obtained
#   from usig the model of Question 27.
#   h(t,x)=hnull*exp(B1*D1*TMS + B2*D1*AGE + B3*D1*G + B4*D2*TMS + B5*D2*AGE + B6*D2*G)
#   D2=0 if patient develops an infection
#   D2=1 if patient dies without infection
#34)For the LM Model of question 33, what is the formula for hte hazar ratio for the group effect G, controlling for TMS and AGE?
#   haz=exp(B3)
#####################################
#Test Questions
#####################################
# The dataset shown below describes a hypothetical study of
# recurrent bladder cancer. The entire dataset contained 53
# patients, each with local bladder cancer tumors who are
# followed for up to 30 months after transurethral surgical
# excision. Three competing risks being considered are local
# recurrence of bladder cancer tumor (event ¼ 1), bladder
# metastasis (event ¼ 2), or other metastasis (event ¼ 3).
# The variable time denotes survival time up to the occurrence
# of one of the three events or censorship from loss to
# follow-up, withdrawal, or end of study. The exposure variable
# of interest is drug treatment status (tx, 0 = placebo,
# 1 = treatment A), The covariates listed here are initial
# number of tumors (num) and initial size of tumors (size)
# in centimeters.
data9<-c("1 1 8 1 1 1
2 0 1 0 1 3
3 0 4 1 2 1
4 0 7 0 1 1
5 0 10 1 5 1
6 2 6 0 4 1
7 0 10 1 4 1
8 0 14 0 1 1
9 0 18 1 1 1
10 3 5 0 1 3
11 0 18 1 1 3
12 1 12 0 1 1
13 2 16 1 1 1
14 0 18 0 1 1
15 0 23 1 3 3
16 3 10 0 1 3
17 1 15 1 1 3
18 0 23 0 1 3
19 2 3 1 1 1
20 3 16 0 1 1
21 1 23 1 1 1
22 1 3 0 3 1
23 2 9 1 3 1
24 2 21 0 3 1
25 0 23 1 3 1
26 3 7 0 2 3
27 3 10 1 2 3
28 1 16 0 2 3
29 1 24 1 2 3
30 1 3 0 1 1
31 2 15 1 1 1
32 2 25 0 1 1
33 0 26 1 1 2
34 1 1 0 8 1
35 0 26 1 8 1
36 1 2 0 1 4
37 1 26 1 1 4
38 1 25 0 1 2
39 0 28 1 1 2
40 0 29 0 1 4
41 0 29 1 1 2
42 0 29 0 4 1
43 3 28 1 1 6
44 1 30 0 1 6
45 2 2 1 1 5
46 1 17 0 1 5
47 1 22 1 1 5
48 0 30 0 1 5
49 3 3 1 2 1
50 2 6 0 2 1
51 3 8 1 2 1
52 3 12 0 2 1
53 0 30 1 2 1")
data9b<-data.frame(matrix(as.numeric(unlist(strsplit(data9, split="\\s|\\n"))), ncol=6,byrow=TRUE))
colnames(data9b)<-c("ID", "Event", "Time", "TX", "NUM", "Size")
#1) Suppose you wish to use these data to determine
#   the effect of tx on survival time for the cause-specific
#   event of a local recurrence of bladder cancer. State a
#   no-interaction Cox PH model for assessing this relationship
#   that adjusts for the covariates num and size.
#   haz(x,t)=hnull(x)exp(B1*TX + B2*NUM + B3*Size); events other than local recurrence (event=1) are considered censored.
#2) When fitting the model given in Question 1, which subjects are considered censored?
#   Bladder metastasis (event=2) and other metastasis (event=3) are considered censored.
#3) How would you modify your answers to Questions 1
#   and 2 if you were interested in the effect of tx on
#   survival time for the cause-specific event of finding
#   metastatic bladder cancer?
#   I would treat local recurrence (event=1) and other mestasis (event=3) as censored and fit essentially the same Cox PH model.
#4) For the model considered in Question 1, briefly
#   describe how to carry out a sensitivity analysis to determine
#   how badly the results from fitting this model
#   might be biased if the assumption of independent competing
#   risks is violated.
#   To see how biased this model might be, we would compare our Cox PH model to those for which the independence assumption has been
#   thoroughly violated. This could be done by considering all events to be the single event type of interest, or by changing the
#   survival times of event types 2 and 3 to the last possible time, essentially treating them as event-free.
#   We would then calculate a Cox PH model for one or both of these assumptions and compare them to the Cox PH model from question 1.
#   This does not give any evidence of dependence or independence, but it does offer a view of how biased our first model might be
#   under worst-case conditions.
#5)
data9_5tx1<-c("0 27 0 0 — — —
2 27 0 0 1 0 0
3 26 0 0 .9630 0 0
4 24 0 0 .8889 0 0
8 23 1 .0435 .8889 .0387 .0387
9 21 0 0 .8116 0 .0387
10 20 0 0 .7729 0 .0387
15 17 1 .0588 .7343 .0432 .0819
16 15 0 0 .6479 0 .0819
18 14 0 0 .6047 0 .0819
22 12 1 .0833 .6047 .0504 .1323
23 11 1 .0910 .5543 .0504 .1827
24 8 1 .1250 .5039 .0630 .2457
26 7 1 .1429 .4409 .0630 .3087
28 4 0 0 .3779 0 .3087
29 2 0 0 .2835 0 .3087
30 1 0 0 .2835 0 .3087")
ds9_5tx1<-data.frame(matrix(as.numeric(unlist(strsplit(data9_5tx1, split="\\s|\\n"))), ncol=7,byrow=TRUE))
colnames(ds9_5tx1)<-c("tf", "nf", "d1f", "h1" ,"S1", "I1", "CIC1")
ds9_5tx1
data9_5tx0<-c("0 26 0 0 — — —
1 26 1 .0400 1 .0400 .0400
2 24 1 .0417 .9615 .0400 .0800
3 23 2 .0870 .9215 .0801 .1601
5 21 0 0 .8413 0 .1601
6 20 0 0 .8013 0 .1601
7 18 0 0 .7212 0 .1601
10 16 0 0 .6811 0 .1601
12 15 1 .0667 .6385 .0426 .2027
14 13 0 0 .6835 0 .2027
16 12 1 .0833 .5534 .0461 .2488
17 10 1 .1000 .4612 .0461 .2949
18 9 0 0 .4150 0 .2949
21 8 0 0 .4150 0 .2949
23 7 0 0 .3632 0 .2949
25 6 1 .1667 .3632 .0605 .3554
29 4 0 0 .2421 0 .3554
30 2 1 0 .2421 0 .3554")
ds9_5tx0<-data.frame(matrix(as.numeric(unlist(strsplit(data9_5tx0, split="\\s|\\n"))), ncol=7,byrow=TRUE))
colnames(ds9_5tx0)<-c("tf", "nf", "d1f", "h1" ,"S1", "I1", "CIC1")
ds9_5tx0
#5A)  Verify the CIC1 calculation provided at failure time
#     tf = 8 for persons in the treatment group (tx = 1);
#     that is, use the original data to compute h1(tf), S(tf1),
#     I1(tf), and CIC1(tf), assuming that the calculations
#     made up to this failure time are correct.
#     h1(tf=8)  = (1/23) =  0.04347
#     S(tf1=8)  = (26/27)*(24/26) = 0.88889
#     I1(tf=8)  = (h1(tf=8))*(S1(tf=8))=(1/23)*(24/27)=0.03865
#     CIC(tf=8) = 0 + 0.03865 = 0.03865
#5B)  Verify the CIC1 calculation provided at failure time
#     tf = 25 for persons in the treatment group (tx = 0).
#     h1(tf=25)  = (1/6) =  0.1667
#     S(tf1=25)  = S(21)*P(tf>23|tf>=23)=(.4150)*(7/8) = 0.36312
#     I1(tf=25)  = (h1(tf=25))*(S1(tf=25))=(1/6)*(0.36312)=0.0605
#     CIC(tf=25) = CIC(tf=23) + I(tf=25) = .2949 + 0.0605
#5C)  Interpret the CIC1 values obtained for both tthe treatment and placebo groups at tf=30.
#     For the treatment group, there is an estimated probability of .3087 that any given individual will have experienced
#     a recurrence of bladder cancer by time tf=30, controlling for the competing risk of bladder or other metastasis.
#     For the placebo group, there is an estimated probability of .3554 that any given individual will have experienced
#     a recurrence of bladder cancer by time tf=30, controlling for local recurrence or other metastasis.
#5D)  How can you calculate the CPC1 values for both treatment and placebo groups at tf=30?
ds5d<-data9b
ds5d$Event1<-0
q
summary(q)
ds5d<-data9b
ds5d$Event1<-0
for(i in 1:length(ds5d[,1])){
ds5d$Event1[i]<-ifelse(ds5d$Event[i]==1, 1, 0)
}
q
ds5d
q<-survfit(Surv(ds5d$Time, ds5d$Event1)~strata(ds5d$TX), data=ds5d)
q
summary(q)
data9b
ds9_5tx0
ds9_5tx1
summary(q)
ds5d
ds9_5tx1
install.packages("cmprsk")
library(comprsk)
library(cmprsk)
data9
data9b
q<-cuminc(data9b$Time, data9b$Event)
q
summary(q)
q<-crr(data9b$Time, data9b$Event)
ds5d
q<-cuminc(ds5d$Time, ds5d$Event1)
summary(q)
q
q<-cuminc(ds5d$Time, ds5d$Event1, strata=ds5dTX)
q<-cuminc(ds5d$Time, ds5d$Event1, strata=ds5d$TX)
q
summary(q)
q
print(q)
print(q)
timepoints(q, times=c(1:30))
q<-cuminc(ds5d$Time, ds5d$Event1, group=ds5d$TX)
q
timepoints(q, times=c(1:30))
data9b
ds5d<-data9b
ds5d
ds5d$Event1<-0
ds5d
for(i in 1:length(ds5d[,1])){
ds5d$Event1[i]<-ifelse(ds5d$Event[i]==1, 1, 0)
}
ds5d
ds7a<-data9b
ds7a$CR<-0
ds7<-data9b
ds7$CR<-0
ds7<-data9b
ds7$CR<-0
for(i in 1:length(ds7)){
ds7$CR[i]<-ifelse(ds7$Event[i]==1, 1, 0)
}
ds7
for(i in 1:length(ds7$CR)){
ds7$CR[i]<-ifelse(ds7$Event[i]==1, 1, 0)
}
ds7
ds7$OM<-0
for(i in 1:length(ds7$CR)){
ds7$CR[i]<-ifelse(ds7$Event[i]==1, 1, 0)
ds7$CM[i]<-ifelse(ds7$Event[i]==2, 1, 0)
ds7$OM[i]<-ifelse(ds7$Event[i]==3, 1, 0)
}
ds7
data9b
ds7
hazard6a<-coxph(Surv(Time, Event1)~tx+num+size, data=ds7)
hazard6a<-coxph(Surv(Time, CR)~tx+num+size, data=ds7)
hazard6a<-coxph(Surv(Time, CR)~TX+NUM+Size, data=ds7)
hazard6a
hazard6b<-coxph(Surv(Time, CM)~TX+NUM+Size, data=ds7)
hazard6c<-coxph(Surv(Time, OM)~TX+NUM+Size, data=ds7)
hazard6a
hazard6b
hazard6c
ds6<-data9b
ds6$CR<-0
ds6$CM<-0
ds6$OM<-0
for(i in 1:length(ds6$CR)){
ds6$CR[i]<-ifelse(ds6$Event[i]==1, 1, 0)
ds6$CM[i]<-ifelse(ds6$Event[i]==2, 1, 0)
ds6$OM[i]<-ifelse(ds6$Event[i]==3, 1, 0)
}
hazard6a<-coxph(Surv(Time, CR)~TX+NUM+Size, data=ds6)
hazard6b<-coxph(Surv(Time, CM)~TX+NUM+Size, data=ds6)
hazard6c<-coxph(Surv(Time, OM)~TX+NUM+Size, data=ds6)
hazard6a
hazard6b
hazard6c
ds6
ds7<-rbind(data9b,data9b, data9b )
ds7
?rep()
rep(1:4,2)
?sort()
?order()
attach(mtcars)
newdata<-mtcars[order(mpg),]
newdata
ds7<-ds7[order(ds7$ID)]
ds7
ds7<-ds7[order(ds7$ID),]
ds7
ds6$CR<-rep(c(1,0,0), 3)
?rep()
ds7$CR<-rep(c(1,0,0), 3)
ds7$CR<-rep(c(1,0,0), length(ds7)/3)
length(ds7)/3
ds7$CR<-rep(c(1,0,0), length(ds7[,1])/3)
ds7$CR
ds7
ds7$CR<-rep(c(1,0,0), length(ds7[,1])/3)
ds7$CM<-rep(c(0,1,0), length(ds7[,1])/3)
ds7$OM<-rep(c(0,0,1), length(ds7[,1])/3)
ds7$OM
ds7
exp(-0.6258+0.6132)
exp(-0.6258+0.2463)
exp(-0.3796)
exp(-0.0127)
data9b
ds7<-rbind(data9b, data9b, data9b)
ds7<-ds7[sort(ds7$ID),]
ds7
ds7<-data9b
data9b
300/(1-.2)
log(0)
log(1)
log(2)/(1.5)
log(2)/2.2
0.462/0.3151
0.462/0.3151
sqrt(4)
2^2
#     NEV=((z[1-a/2] + z[1-b/2])(RDelta+1)/(sqrt(R)*(Delta-1)))^2 = ((1.96 + 0.84)((2)(1.467)+1)/(sqrt(2)*((1.467)-1)))^2
((1.96 + 0.84)((2)(1.467)+1)/(sqrt(2)*((1.467)-1)))^2
(1.96 + 0.84)((2)(1.467)+1)
((1.96 + 0.84)((2)*(1.467)+1)/(sqrt(2)*((1.467)-1)))^2
((2)*(1.467)+1)
((1.96 + 0.84)*((2)*(1.467)+1)/(sqrt(2)*((1.467)-1)))^2
((1.96 + 0.84)*(((2)*(1.467))+1)/(sqrt(2)*((1.467)-1)))^2
(1 - (1/(0.462*2))*(exp(-0.462*3)-exp(-0.462(2+3))))
(1 - (1/(0.462*2))*(exp(-0.462*3)-exp(-0.462*(2+3))))
(1 - (1/(0.3151*2))*(exp(-0.3151*3)-exp(-0.3151*(2+3))))
((1.96 + 0.84)*(((2)*(1.467))+1)/(sqrt(2)*((1.467)-1)))^2
(278.1769)/(((2/(2+1))*(0.7117)) + ((1/(2+1))*(0.8368)))
(130)/(((2/(2+1))*(0.7117)) + ((1/(2+1))*(0.8368)))
(279)/(((2/(2+1))*(0.7117)) + ((1/(2+1))*(0.8368)))
(2/2+1)*(173)
(2/(2+1))*(173)
(2/(2+1))*(371)
(247.3)/2
(371)/(1-0.25)
(2/(2+1))*(495)
(330)/2
495/(1 - 0.05 - 0.1)
(2/(2+1))*(583)
(389)/2
194+389
((1.96 + 1.282)*(((2)*(2))+1)/(sqrt(2)*((2)-1)))^2
(132)/(((2/(2+1))*(1-exp(-2*0.05*5))) + ((1/(2+1))*(1-exp(-2*5))))
(132)/(((2/(2+1))*(1-exp(-2*0.05*3))) + ((1/(2+1))*(1-exp(-2*3))))
(132)/(((2/(2+1))*(1-exp(-2*0.05*3))) + ((1/(2+1))*(1-exp(-.05*3))))
(2/(2+1))*(602)
(401)/2
(1 - (1/(0.1*2))*(exp(-0.1*2)-exp(-0.1*(2+2))))
(1 - (1/(0.05*2))*(exp(-0.05*2)-exp(-0.1*(2+2))))
(1 - (1/(0.05*2))*(exp(-0.05*2)-exp(-0.05*(2+2))))
((1.96 + 1.282)*(((2)*(2))+1)/(sqrt(2)*((2)-1)))^2
(132)/(((2/(2+1))*(0.1389)) + ((1/(2+1))*(0.2579)))
(2/(2+1))*(739)
(492)/2
(739)/(1-0.25)
739/(1 - 0.05 - 0.1)
(495/(1 - 0.05 - 0.1))^2
495/((1 - 0.05 - 0.1)^2)
495/(((1 - 0.05) - 0.1)^2)
739/((1 - 0.05 - 0.1)^2)
(2/(2+1))*(986)
(658)/2
739/((1 - 0.05 - 0.1)^2)
986/((1 - 0.05 - 0.1)^2)
(2/(2+1))*(1365)
910/2
(2/(2+1))*(583)
495/(((1 - 0.05) - 0.1)^2)
(371)/(1-0.25)
(279)/(((2/(2+1))*(0.7117)) + ((1/(2+1))*(0.8368)))
install.packages("mirt.R")
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
# this will start the updating process of your R installation.  It will check for newer versions, and
# if one is available, will guide you through the decisions you'd need to make.
updateR()
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
install.packages("installr")
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
install.packages("installr")
# using the package:
# this will start the updating process of your R installation.  It will check for newer versions, and
# if one is available, will guide you through the decisions you'd need to make.
updateR()
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
install.packages("installr")
# using the package:
# this will start the updating process of your R installation.  It will check for newer versions, and
# if one is available, will guide you through the decisions you'd need to make.
updateR()
dsCity<-c("Oklahoma city", "Tulsa", "Norman")
install.packages("maps")
library(maps)
data("countyMapEnv")
dsMaps<-data("countyMapEnv")
View(dsMaps)
dsMaps<-data(countyMapEnv)
View(dsMaps)
data(countyMapEnv)
map('county', 'iowa', fill = TRUE, col = palette())
?map()
map('county', 'oklahoma', fill = TRUE, col = palette())
map('county', 'oklahoma', fill = TRUE, col = palette())
okmap<-map('county', 'oklahoma', fill = TRUE, col = palette())
str(okmap)
okmap$names
?maps()
?map()
okmap<-map('county', 'oklahoma', fill = TRUE, col = palette(), namesonly = TRUE)
okmap<-map('county', 'oklahoma', fill = FALSE, col = palette(), namesonly = TRUE)
?map()
length(okmap$names)
okmap$names
okmap<-map('county', 'oklahoma', fill = FALSE, col = palette(), namesonly = TRUE)
okmap$names
okmap<-map('county', 'oklahoma', fill = FALSE, col = palette())
okmap$names
length(okmap$names)
?substr()
substr(rep("abcdef", 4), 1:4, 4:5)
x <- c("asfef", "qwerty", "yuiop[", "b", "stuff.blah.yech")
x
substr(x, 2, 5)
okcounties<-substr(okmap$names, 1, 10)
okcounties
okcounties<-substr(okmap$names, 1, 9)
okcounties
okcounties<-substr(okmap$names, 9)
okcounties<-substr(okmap$names, 9, last)
okcounties<-substr(okmap$names, 9, 20)
okcounties
okcounties<-substr(okmap$names, 10, 30)
okcounties
length(okcounties)
setwd("Documents/GitHub/Utilities")
getwd()
>>>>>>> Stashed changes
######################################
# Setting up
######################################
rm(list=ls())
<<<<<<< Updated upstream
setwd("C:/Users/Andrew/Documents/GitHub/Utilities")
######################################
# Reading in datasets
######################################
# Reading in List of cities by state and zip
# https://www.zip-codes.com/state/ok.asp
ds1<-read.csv("OKCitiesZipCounty.csv")
# County by Income
# "SELECTED ECONOMIC CHARACTERISTICS 2006-2010 American Community Survey 5-Year Estimates". U.S. Census Bureau. Retrieved 2012-11-25.
# "Profile of General Population and Housing Characteristics: 2010 Demographic Profile Data". U.S. Census Bureau. Archived from the original on March 5, 2014. Retrieved 2012-11-25.
ds2<-read.csv("CountyxIncome.csv")
# Zip codes by income, sorta.
# https://www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-2014-zip-code-data-soi
ds3<-read.csv("IRSIncomeTax.csv")
ds4<-read.csv("OKChildCare.csv")
View(ds4)
View(ds1)
?substr()
head(ds1)
ds12<-ds1
substr(ds12$ZIP.Code, start=1, stop=9)<-""
substr(ds12$ZIP.Code, start=1, stop=9)
substr(ds12$ZIP.Code, start=10, stop=14)
ds12$ZIP.Code<-substr(ds12$ZIP.Code, start=10, stop=14)
head(ds1)
head(ds12)
ds1$ZIP.Code<-substr(ds1$ZIP.Code, start=10, stop=14)
head(ds12)
head(ds1)
colnames(ds1)
colnames(ds1) <- c("zip", "type", "city", "county", "area")
colnames(ds1)
View(ds1)
write.csv(ds1, "ds1")
write.csv(ds1, "ds1.csv")
table(ds4$Star_Level_Cat)
table(ds4$Star_Level_Cat, useNA="ifany")
=======
setwd("/Users/Andrew/Documents/Github/Utilities")
library(maps)
county.fips
head(county.fips)
ds<-county.fips
head(ds)
ds$state<-ds$polyname
ds$state<-substr(ds$state, 1, 8)
head(ds)
ds<-ds[ds$state=="oklahoma"]
ds<-ds[ds$state=="oklahoma", ]
ds
>>>>>>> Stashed changes
######################################
# Setting up
######################################
rm(list=ls())
<<<<<<< Updated upstream
setwd("C:/Users/Andrew/Documents/GitHub/Utilities")
######################################
# Reading in datasets
######################################
# Reading in List of cities by state and zip
# https://www.zip-codes.com/state/ok.asp
ds1<-read.csv("OKCitiesZipCounty.csv")
ds1$ZIP.Code<-substr(ds1$ZIP.Code, start=10, stop=14)
colnames(ds1) <- c("zip", "type", "city", "county", "area")
write.csv(ds1, "dsOKCitiesZipCounty.csv")
=======
setwd("/Users/Andrew/Documents/Github/Utilities")
library(maps)
######################################
# Messing around
######################################
dsCity<-c("Oklahoma city", "Tulsa", "Norman")
okmap<-map('county', 'oklahoma', fill = FALSE, col = palette())
okmap$names
okcounties<-substr(okmap$names, 10, 30)
okcounties
ds<-county.fips
head(ds)
ds$state<-ds$polyname
ds$state<-substr(ds$state, 1, 8)
ds<-ds[ds$state=="oklahoma", ]
head(ds)
install.packes("USCensus2010")
install.packages("USCensus2010")
yes
install.packages("UScensus2010")
)
library(UScensus2010)
install.packages("sp")
library(sp)
portland<-city(name="portland",state="or")
library(UScensus2010)
portland<-city(name="portland",state="or")
?city()
>>>>>>> Stashed changes
